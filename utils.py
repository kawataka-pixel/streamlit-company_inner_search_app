"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ±ç”¨å‡¦ç†ã‚’ã¾ã¨ã‚ãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã§ã™ã€‚
"""

from dotenv import load_dotenv
import streamlit as st

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser

import constants as ct


# .env ã«å®šç¾©ã•ã‚ŒãŸç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


def get_source_icon(source: str) -> str:
    """
    å‚ç…§å…ƒï¼ˆãƒ•ã‚¡ã‚¤ãƒ« / URLï¼‰ã«å¿œã˜ãŸè¡¨ç¤ºç”¨ã‚¢ã‚¤ã‚³ãƒ³ã‚’è¿”ã™ã€‚
    """
    if not source or not str(source).strip():
        return ct.WARNING_ICON

    src = str(source).strip().lower()
    if src.startswith("http://") or src.startswith("https://"):
        return ct.LINK_SOURCE_ICON
    return ct.DOC_SOURCE_ICON


def build_error_message(message: str) -> str:
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨å…±é€šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é€£çµã—ã¦è¿”ã™ã€‚
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])


def _ensure_chat_history():
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []


def _build_independent_query(llm: ChatOpenAI, chat_message: str) -> str:
    """
    ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚ç†è§£ã§ãã‚‹ç‹¬ç«‹ã—ãŸã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    chain = prompt | llm | StrOutputParser()
    return chain.invoke(
        {
            "input": chat_message,
            "chat_history": st.session_state.chat_history,
        }
    )


def _generate_answer(llm: ChatOpenAI, chat_message: str, context_text: str) -> str:
    """
    RAGã§å–å¾—ã—ãŸæ–‡è„ˆ + ä¼šè©±å±¥æ­´ã‚’ã‚‚ã¨ã«å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    qa_system = (
        ct.SYSTEM_PROMPT_DOC_SEARCH
        if st.session_state.mode == ct.ANSWER_MODE_1
        else ct.SYSTEM_PROMPT_INQUIRY
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system),
            MessagesPlaceholder("chat_history"),
            ("human", "è³ªå•: {input}\n\nå‚ç…§æ–‡æ›¸:\n{context}"),
        ]
    )
    answer_chain = qa_prompt | llm | StrOutputParser()
    return answer_chain.invoke(
        {
            "input": chat_message,
            "chat_history": st.session_state.chat_history,
            "context": context_text,
        }
    )


def get_llm_response(chat_message: str):
    """
    LangChain 1.x ã® Runnable API ã‚’ç”¨ã„ã¦ RAG å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚

    Returns:
        {"answer": str, "context": List[Document]}
    """
    _ensure_chat_history()

    retriever = st.session_state.get("retriever")
    if retriever is None:
        st.error("ğŸ” æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆretrieverï¼‰ãŒæœªåˆæœŸåŒ–ã§ã™ã€‚initialize.py ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return {
            "answer": "ç¾åœ¨ã€æ¤œç´¢æº–å‚™ä¸­ã§ã™ã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚",
            "context": [],
        }

    llm = ChatOpenAI(model=ct.MODEL, temperature=ct.TEMPERATURE)

    refined_query = _build_independent_query(llm, chat_message)

    docs_for_answer = retriever.invoke(refined_query)
    if not docs_for_answer:
        # è³ªå•ã®å¤‰å½¢ã§ãƒ’ãƒƒãƒˆã—ãªã„å ´åˆã¯å…ƒã®å…¥åŠ›ã§å†æ¤œç´¢
        docs_for_answer = retriever.invoke(chat_message)

    context_text = "\n\n".join(doc.page_content for doc in docs_for_answer)

    answer = _generate_answer(llm, chat_message, context_text)

    st.session_state.chat_history.extend(
        [
            HumanMessage(content=chat_message),
            AIMessage(content=answer),
        ]
    )

    return {"answer": answer, "context": docs_for_answer}
