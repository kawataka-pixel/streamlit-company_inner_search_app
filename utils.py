"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
from dotenv import load_dotenv
import streamlit as st

# ==========================================================
# LangChainï¼ˆæœ€æ–°ç‰ˆ0.3ç³»ï¼‰å¯¾å¿œã®æ–°ã—ã„ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ§‹æˆ
# ==========================================================
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

import constants as ct

############################################################
# è¨­å®šé–¢é€£
############################################################
load_dotenv()

# ï¼ˆä»»æ„ï¼‰èµ·å‹•æ™‚ã«å±¥æ­´ã ã‘åˆæœŸåŒ–ã—ã¦ãŠãã¨å®‰å¿ƒ
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

############################################################
# é–¢æ•°å®šç¾©
############################################################
def get_llm_response(chat_message: str):
    """
    LangChain 1.x æ§‹æˆï¼ˆRunnableï¼‰ç‰ˆã€‚
    å±¥æ­´ã‚’è¦‹ã¦è³ªå•ã‚’ç‹¬ç«‹åŒ– â†’ retrieveræ¤œç´¢ â†’ å›ç­”ç”Ÿæˆã€‚
    è¿”ã‚Šå€¤: {"answer": str, "context": List[Document]}
    """
    # --- å®‰å…¨ã‚¬ãƒ¼ãƒ‰ï¼šã“ã“ã¯å¿…ãšé–¢æ•°å†…ã§ï¼ ---
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    if "retriever" not in st.session_state or st.session_state.retriever is None:
        st.error("ğŸ” æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆretrieverï¼‰ãŒæœªåˆæœŸåŒ–ã§ã™ã€‚initialize.py ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return {"answer": "ç¾åœ¨ã€æ¤œç´¢æº–å‚™ä¸­ã§ã™ã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚", "context": []}

    # --- LLM ---
    llm = ChatOpenAI(model=ct.MODEL, temperature=ct.TEMPERATURE)

    # --- â‘  å±¥æ­´ã‚’è¦‹ã¦è³ªå•ã‚’ç‹¬ç«‹åŒ–ï¼ˆæ—§ create_history_aware_retriever ç›¸å½“ï¼‰ ---
    reform_prompt = ChatPromptTemplate.from_messages([
        ("system", ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    refine_chain = reform_prompt | llm | StrOutputParser()   # -> refined_query(str)

    # --- â‘¡ æ¤œç´¢ï¼ˆrefined_query ã‚’ retriever ã«æ¸¡ã™ï¼‰ ---
    def _retrieve(inputs):
        q = inputs["refined_query"]
        return st.session_state.retriever.get_relevant_documents(q)

    # --- â‘¢ å›ç­”ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥ï¼‰ ---
    qa_system = ct.SYSTEM_PROMPT_DOC_SEARCH if st.session_state.mode == ct.ANSWER_MODE_1 else ct.SYSTEM_PROMPT_INQUIRY
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system),
        MessagesPlaceholder("chat_history"),
        ("human", "è³ªå•: {input}\n\nå‚è€ƒæ–‡æ›¸:\n{context}")
    ])

    def _format_docs(docs):
        return "\n\n".join(d.page_content for d in docs)

    # --- Runnableã‚°ãƒ©ãƒ•ï¼ˆæ—§ create_retrieval_chain / create_stuff_documents_chain ã®ä»£æ›¿ï¼‰ ---
    chain = (
        {
            "chat_history": RunnablePassthrough(),
            "input": RunnablePassthrough(),
            "refined_query": refine_chain,
        }
        | {
            "chat_history": lambda x: x["chat_history"],
            "input":        lambda x: x["input"],
            "context":      lambda x: _format_docs(_retrieve(x)),
        }
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    answer = chain.invoke({
        "input": chat_message,
        "chat_history": st.session_state.chat_history
    })

    # å±¥æ­´ã«è¿½åŠ 
    st.session_state.chat_history.extend([
        HumanMessage(content=chat_message),
        AIMessage(content=answer),
    ])

    # ã‚½ãƒ¼ã‚¹è¡¨ç¤ºç”¨ï¼ˆä¸è¦ãªã‚‰å‰Šé™¤OKï¼‰
    docs = st.session_state.retriever.get_relevant_documents(chat_message)
    return {"answer": answer, "context": docs}
